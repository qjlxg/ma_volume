import pandas as pd
import os
import glob
from datetime import datetime
import pytz
import multiprocessing as mp

# --- é…ç½® ---
STOCK_DATA_DIR = 'stock_data'
STOCK_NAMES_FILE = 'stock_names.csv'
MIN_CLOSE_PRICE = 5.0

# è®¾ç½®ä¸Šæµ·æ—¶åŒº
SH_TZ = pytz.timezone('Asia/Shanghai')

# å®šä¹‰ä¸­æ–‡åˆ—ååˆ°è‹±æ–‡æ ‡å‡†åˆ—åçš„æ˜ å°„ (åŸºäºç”¨æˆ·æä¾›çš„æ ¼å¼)
COLUMN_MAPPING = {
    'æ—¥æœŸ': 'Date',
    'å¼€ç›˜': 'Open',
    'æ”¶ç›˜': 'Close',
    'æœ€é«˜': 'High',
    'æœ€ä½': 'Low',
    'æˆäº¤é‡': 'Volume',
    'æˆäº¤é¢': 'Amount',
    'è‚¡ç¥¨ä»£ç ': 'Code' 
}

def is_stacked_multi_cannon(df):
    """
    åˆ¤æ–­ K çº¿æ•°æ®ï¼ˆä¾èµ–äºé‡å‘½ååçš„è‹±æ–‡åˆ—åï¼šOpen, Close, High, Lowï¼‰æ˜¯å¦å½¢æˆäº†
    â€œå å½¢å¤šæ–¹ç‚®â€å½¢æ€ã€‚
    
    å½¢æ€é‡åŒ–é€»è¾‘ï¼ˆå…³æ³¨æœ€è¿‘ 4 ä¸ªäº¤æ˜“æ—¥ K1, K2, K3, K4ï¼‰ï¼š
    1. K2, K3 å¿…é¡»æ˜¯é˜³çº¿ (Close > Open)ã€‚
    2. K4 å¿…é¡»æ˜¯çªç ´å¤§é˜³çº¿ (Close > Open)ã€‚
    3. K2, K3 å®ä½“ç›¸å¯¹è¾ƒå°ï¼ˆå°äº K4 å®ä½“çš„ä¸€åŠï¼‰ï¼Œå½¢æˆæ•´ç†ã€‚
    4. K4 çš„æ”¶ç›˜ä»·å¿…é¡»çªç ´ K1, K2, K3 çš„æœ€é«˜ä»·ã€‚
    5. K4 çš„æ”¶ç›˜ä»·å¿…é¡» >= MIN_CLOSE_PRICE (5.0å…ƒ)ã€‚
    """
    if len(df) < 4:
        return False

    # å–æœ€è¿‘çš„ 4 æ ¹ K çº¿
    df_recent = df.iloc[-4:]
    
    # æ£€æŸ¥æ‰€æœ‰å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ['Open', 'Close', 'High', 'Low']
    if not all(col in df_recent.columns for col in required_cols):
        return False
    
    O, C, H, L = df_recent['Open'].values, df_recent['Close'].values, df_recent['High'].values, df_recent['Low'].values
    
    # K1, K2, K3, K4 çš„ç´¢å¼•æ˜¯ 0, 1, 2, 3

    # 1. K2 å’Œ K3 å¿…é¡»æ˜¯é˜³çº¿ï¼ˆClose > Openï¼‰
    is_k2_up = C[1] > O[1]
    is_k3_up = C[2] > O[2]
    if not (is_k2_up and is_k3_up):
        return False

    # 2. K4 å¿…é¡»æ˜¯çªç ´å¤§é˜³çº¿ï¼ˆClose > Openï¼‰
    is_k4_up = C[3] > O[3]
    if not is_k4_up:
        return False

    # 3. K2, K3 å½¢æˆæ•´ç†æˆ–å å‡ï¼Œå®ä½“ç›¸å¯¹è¾ƒå°
    k2_body_size = abs(C[1] - O[1])
    k3_body_size = abs(C[2] - O[2])
    k4_body_size = abs(C[3] - O[3])
    
    if not (k2_body_size < 0.5 * k4_body_size and k3_body_size < 0.5 * k4_body_size):
        return False

    # 4. K4 çªç ´ K1, K2, K3 çš„æœ€é«˜ä»·
    max_prev_high = max(H[0], H[1], H[2])
    
    # K4 çš„æ”¶ç›˜ä»·å¿…é¡»çªç ´å‰ä¸‰æ ¹ K çº¿çš„æœ€é«˜ä»·
    if C[3] <= max_prev_high:
        return False
        
    # 5. K4 çš„æœ€æ–°æ”¶ç›˜ä»·è¿‡æ»¤
    if C[3] < MIN_CLOSE_PRICE:
        return False

    return True


def process_single_file(file_path):
    """å¤„ç†å•ä¸ªè‚¡ç¥¨æ•°æ®æ–‡ä»¶ï¼Œæ£€æŸ¥å½¢æ€å¹¶è¿”å›ä»£ç ï¼ˆå¦‚æœç¬¦åˆï¼‰"""
    stock_code = os.path.basename(file_path).split('.')[0]
    try:
        df = pd.read_csv(file_path)
        
        # 1. é‡å‘½ååˆ—ä»¥é€‚åº”è„šæœ¬é€»è¾‘
        df = df.rename(columns=COLUMN_MAPPING)
        
        # 2. æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å·²æˆåŠŸé‡å‘½åå¹¶å­˜åœ¨
        required_cols = ['Date', 'Open', 'Close', 'High', 'Low']
        if not all(col in df.columns for col in required_cols):
             # æ£€æŸ¥æ˜¯å¦å› ä¸ºåŸå§‹æ–‡ä»¶ç¼ºå¤±åˆ—
            missing_cols_cn = [col_cn for col_cn, col_en in COLUMN_MAPPING.items() if col_en in required_cols and col_cn not in df.columns]
            if missing_cols_cn:
                # print(f"âŒ æ–‡ä»¶ {file_path} ç¼ºå°‘åŸå§‹åˆ—: {', '.join(missing_cols_cn)}")
                pass # å‡å°‘æ—¥å¿—è¾“å‡ºï¼ŒåªæŠ¥å‘Šæœ€ç»ˆç»“æœ
            return None
        
        # 3. è§£ææ—¥æœŸå¹¶æ¸…ç† NaN
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df = df.dropna(subset=['Date', 'Open', 'Close', 'High', 'Low']) # ç§»é™¤æ— æ•ˆæ•°æ®è¡Œ

        # 4. ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
        df = df.sort_values(by='Date').reset_index(drop=True)

        if is_stacked_multi_cannon(df):
            return stock_code
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} å‡ºé”™: {e}")
        
    return None

def main():
    print(f"--- è‚¡ç¥¨å½¢æ€æ‰«æå™¨å¯åŠ¨ ({datetime.now(SH_TZ).strftime('%Y-%m-%d %H:%M:%S')}) ---")
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶
    all_files = glob.glob(os.path.join(STOCK_DATA_DIR, '*.csv'))
    if not all_files:
        print(f"æœªåœ¨ '{STOCK_DATA_DIR}' ç›®å½•ä¸‹æ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ•°æ®å·²ä¸Šä¼ ã€‚")
        return

    # 2. å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
    print(f"å¼€å§‹æ‰«æ {len(all_files)} ä¸ªè‚¡ç¥¨æ–‡ä»¶...")
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ CPU æ ¸å¿ƒè¿›è¡Œå¹¶è¡Œå¤„ç†
    pool = mp.Pool(mp.cpu_count())
    found_codes = pool.map(process_single_file, all_files)
    pool.close()
    pool.join()
    
    # è¿‡æ»¤æ‰ None å€¼
    found_codes = [code for code in found_codes if code is not None]
    
    if not found_codes:
        print("æœªæ‰¾åˆ°ç¬¦åˆ 'å å½¢å¤šæ–¹ç‚®' å½¢æ€çš„è‚¡ç¥¨ã€‚")
        return

    # 3. åŒ¹é…è‚¡ç¥¨åç§°
    print(f"å…±å‘ç° {len(found_codes)} åªç¬¦åˆå½¢æ€çš„è‚¡ç¥¨ï¼Œå¼€å§‹åŒ¹é…åç§°...")
    try:
        # å‡è®¾ stock_names.csv æ ¼å¼ä¸º 'code', 'name'
        names_df = pd.read_csv(STOCK_NAMES_FILE, dtype={'code': str})
        names_df = names_df.set_index('code')['name'].to_dict()
    except Exception as e:
        print(f"è¯»å–æˆ–å¤„ç† '{STOCK_NAMES_FILE}' æ–‡ä»¶å¤±è´¥: {e}ã€‚å°†åªè¾“å‡ºä»£ç ã€‚")
        names_df = {}
        
    # 4. ç»„ç»‡ç»“æœ
    results = []
    for code in found_codes:
        name = names_df.get(code, 'åç§°æœªæ‰¾åˆ°')
        results.append({'è‚¡ç¥¨ä»£ç ': code, 'è‚¡ç¥¨åç§°': name})
        
    results_df = pd.DataFrame(results)

    # 5. ä¿å­˜ç»“æœ
    now = datetime.now(SH_TZ)
    timestamp_str = now.strftime('%Y%m%d_%H%M%S')
    year_month_dir = now.strftime('%Y/%m')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join('scan_results', year_month_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    # ç»“æœæ–‡ä»¶å
    output_filename = f'stacked_multi_cannon_{timestamp_str}.csv'
    output_path = os.path.join(output_dir, output_filename)
    
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"\nğŸ‰ ç­›é€‰ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    # ä½¿ç”¨ try-except æ•è·ä¸»ç¨‹åºå¼‚å¸¸ï¼Œç¡®ä¿æ—¥å¿—å‹å¥½
    try:
        main()
    except Exception as e:
        print(f"ä¸»ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
